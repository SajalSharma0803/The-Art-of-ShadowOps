{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4043617,"sourceType":"datasetVersion","datasetId":2395063},{"sourceId":9133599,"sourceType":"datasetVersion","datasetId":5514920},{"sourceId":9133661,"sourceType":"datasetVersion","datasetId":5514970},{"sourceId":9133656,"sourceType":"datasetVersion","datasetId":5514966}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Flatten\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Load the VGG16 model pre-trained on ImageNet, excluding the top layers\nbase_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\n# Freeze the convolutional base to prevent training\nfor layer in base_model.layers:\n    layer.trainable = False\n\n# Add custom layers on top\nx = Flatten()(base_model.output)       # Flatten the output of the convolutional base\nx = Dense(512, activation='relu')(x)   # Fully connected layer with 512 units and ReLU activation\nx = Dense(1, activation='sigmoid')(x)  # Output layer with sigmoid activation for binary classification\n\n# Create the final model\nmodel = Model(inputs=base_model.input, outputs=x)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Data augmentation and rescaling\ndatagen = ImageDataGenerator(\n    rescale=1./255,          # Rescale pixel values to [0, 1]\n    validation_split=0.2     # Split 20% of data for validation\n)\n\n# Training data generator\ntrain_generator = datagen.flow_from_directory(\n    '/kaggle/input/stegoimagesdataset',  # Update with the actual path to your dataset\n    target_size=(224, 224),               # Resize images to 224x224\n    batch_size=32,                        # Batch size\n    class_mode='binary',                  # Binary classification\n    subset='training'                     # Use the 'training' subset\n)\n\n# Validation data generator\nvalidation_generator = datagen.flow_from_directory(\n    '/kaggle/input/stegoimagesdataset',  # Update with the actual path to your dataset\n    target_size=(224, 224),               # Resize images to 224x224\n    batch_size=32,                        # Batch size\n    class_mode='binary',                  # Binary classification\n    subset='validation'                  # Use the 'validation' subset\n)\n\n# Train the model\nhistory = model.fit(\n    train_generator,\n    epochs=10,                           # Number of epochs\n    validation_data=validation_generator\n)\n\n# Save the model if needed\nmodel.save('/kaggle/working/vgg16_finetuned_model.h5')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-08T16:04:37.017700Z","iopub.execute_input":"2024-08-08T16:04:37.018713Z","iopub.status.idle":"2024-08-08T16:49:47.593042Z","shell.execute_reply.started":"2024-08-08T16:04:37.018668Z","shell.execute_reply":"2024-08-08T16:49:47.592246Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Found 35200 images belonging to 3 classes.\nFound 8800 images belonging to 3 classes.\nEpoch 1/10\n\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m275s\u001b[0m 247ms/step - accuracy: 0.5277 - loss: -12.4168 - val_accuracy: 0.5895 - val_loss: -136.7496\nEpoch 2/10\n\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m270s\u001b[0m 244ms/step - accuracy: 0.5805 - loss: -392.3898 - val_accuracy: 0.5731 - val_loss: -736.6566\nEpoch 3/10\n\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m272s\u001b[0m 246ms/step - accuracy: 0.5936 - loss: -1540.7452 - val_accuracy: 0.6595 - val_loss: -1931.7002\nEpoch 4/10\n\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 242ms/step - accuracy: 0.5956 - loss: -3452.9163 - val_accuracy: 0.5993 - val_loss: -3529.1934\nEpoch 5/10\n\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 243ms/step - accuracy: 0.5956 - loss: -5973.7441 - val_accuracy: 0.6828 - val_loss: -5413.2041\nEpoch 6/10\n\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m270s\u001b[0m 245ms/step - accuracy: 0.5974 - loss: -9188.8984 - val_accuracy: 0.6282 - val_loss: -7823.7910\nEpoch 7/10\n\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 243ms/step - accuracy: 0.5991 - loss: -12415.9287 - val_accuracy: 0.6862 - val_loss: -10426.8486\nEpoch 8/10\n\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m270s\u001b[0m 244ms/step - accuracy: 0.6078 - loss: -16343.3799 - val_accuracy: 0.6148 - val_loss: -13435.1338\nEpoch 9/10\n\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 240ms/step - accuracy: 0.6011 - loss: -21338.5234 - val_accuracy: 0.6384 - val_loss: -16876.7910\nEpoch 10/10\n\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m271s\u001b[0m 245ms/step - accuracy: 0.5986 - loss: -27343.1836 - val_accuracy: 0.6561 - val_loss: -20428.9453\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing import image\nimport numpy as np\n\ndef detect_steganographic_image(image_path, model_path):\n    # Load the trained model\n    model = load_model(model_path)\n    \n    # Load and preprocess the image\n    img = image.load_img(image_path, target_size=(224, 224))\n    img_array = image.img_to_array(img) / 255.0  # Normalize pixel values to [0, 1]\n    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n    \n    # Make a prediction\n    prediction = model.predict(img_array)\n    \n    # Determine the class based on the prediction\n    if prediction[0] > 0.5:\n        print(\"The image is steganographic.\")\n    else:\n        print(\"The image is normal.\")\n\n# Example usage with uploaded files\nimage_path = '/kaggle/working/your_uploaded_image.jpg'\nmodel_path = '/kaggle/working/fine_tuned_model.h5'\n\ndetect_steganographic_image('/kaggle/input/sajal042/pngtree-normal-icon-png-image_6630696.jpg', '/kaggle/working/vgg16_finetuned_model.h5')\ndetect_steganographic_image('/kaggle/input/sajal2/cardad.jpg', '/kaggle/working/vgg16_finetuned_model.h5')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-08T17:15:02.952059Z","iopub.execute_input":"2024-08-08T17:15:02.952688Z","iopub.status.idle":"2024-08-08T17:15:04.700750Z","shell.execute_reply.started":"2024-08-08T17:15:02.952658Z","shell.execute_reply":"2024-08-08T17:15:04.699865Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 527ms/step\nThe image is normal.\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 534ms/step\nThe image is steganographic.\n","output_type":"stream"}]}]}